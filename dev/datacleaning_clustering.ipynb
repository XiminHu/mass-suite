{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster,mixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = pd.read_csv('../example_data/clustering/sample1114.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Average Rt(min)  Average Mz  S/N average  20181114_CEC_CAL-8-no4_MSpos_1  \\\n0            8.381   100.03931        60.33                               0   \n1            2.332   100.07604        88.07                             412   \n2            2.544   100.11243       111.09                            4168   \n3            5.359   100.11253       155.25                            1239   \n4            0.628   101.00802        20.59                            1295   \n\n   20181114_CEC_CAL-8-no4_MSpos_2  20181114_CEC_CAL-8-no4_MSpos_3  \\\n0                               0                               0   \n1                             754                            1985   \n2                            3547                            2459   \n3                            1145                            1007   \n4                            1499                            1508   \n\n   20181114_CEC_CAL-8-no4_MSpos_4  20181114_CEC_CAL-8-no4_MSpos_5  \\\n0                               0                               0   \n1                            1639                            2049   \n2                            3768                            3544   \n3                             550                            1254   \n4                            2067                            2103   \n\n   20181114_CEC_CAL-8-no4_MSpos_6  20181114_CEC_CAL-8-no4_MSpos_7  ...  \\\n0                               0                               0  ...   \n1                            1796                            2702  ...   \n2                            1695                            2862  ...   \n3                             781                            1714  ...   \n4                            2153                            1500  ...   \n\n   20181114_SR520-Creek_Mix6A_3  20181114_SR520-Creek_Mix6B_1  \\\n0                            68                            82   \n1                           674                          1609   \n2                          2282                          1246   \n3                        203706                        231624   \n4                         18334                         14693   \n\n   20181114_SR520-Creek_Mix6B_2  20181114_SR520-Creek_Mix6B_3  \\\n0                           100                             0   \n1                          1571                           782   \n2                          1662                          2120   \n3                        152532                        231635   \n4                         11126                         10754   \n\n   20181114_SwanCreek-Dec_1  20181114_SwanCreek-Dec_2  \\\n0                         0                         0   \n1                       976                       729   \n2                       840                      1336   \n3                    260914                    258902   \n4                     14120                     10229   \n\n   20181114_SwanCreek-Dec_3  20181114_SwanCreek-May_1  \\\n0                         0                         0   \n1                       587                      6437   \n2                      1665                      1200   \n3                    234764                    234498   \n4                     12813                     10449   \n\n   20181114_SwanCreek-May_2  20181114_SwanCreek-May_3  \n0                         0                         0  \n1                      3174                      2708  \n2                      1191                      1217  \n3                    193185                    193974  \n4                      9869                     11369  \n\n[5 rows x 157 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Average Rt(min)</th>\n      <th>Average Mz</th>\n      <th>S/N average</th>\n      <th>20181114_CEC_CAL-8-no4_MSpos_1</th>\n      <th>20181114_CEC_CAL-8-no4_MSpos_2</th>\n      <th>20181114_CEC_CAL-8-no4_MSpos_3</th>\n      <th>20181114_CEC_CAL-8-no4_MSpos_4</th>\n      <th>20181114_CEC_CAL-8-no4_MSpos_5</th>\n      <th>20181114_CEC_CAL-8-no4_MSpos_6</th>\n      <th>20181114_CEC_CAL-8-no4_MSpos_7</th>\n      <th>...</th>\n      <th>20181114_SR520-Creek_Mix6A_3</th>\n      <th>20181114_SR520-Creek_Mix6B_1</th>\n      <th>20181114_SR520-Creek_Mix6B_2</th>\n      <th>20181114_SR520-Creek_Mix6B_3</th>\n      <th>20181114_SwanCreek-Dec_1</th>\n      <th>20181114_SwanCreek-Dec_2</th>\n      <th>20181114_SwanCreek-Dec_3</th>\n      <th>20181114_SwanCreek-May_1</th>\n      <th>20181114_SwanCreek-May_2</th>\n      <th>20181114_SwanCreek-May_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.381</td>\n      <td>100.03931</td>\n      <td>60.33</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>68</td>\n      <td>82</td>\n      <td>100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.332</td>\n      <td>100.07604</td>\n      <td>88.07</td>\n      <td>412</td>\n      <td>754</td>\n      <td>1985</td>\n      <td>1639</td>\n      <td>2049</td>\n      <td>1796</td>\n      <td>2702</td>\n      <td>...</td>\n      <td>674</td>\n      <td>1609</td>\n      <td>1571</td>\n      <td>782</td>\n      <td>976</td>\n      <td>729</td>\n      <td>587</td>\n      <td>6437</td>\n      <td>3174</td>\n      <td>2708</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.544</td>\n      <td>100.11243</td>\n      <td>111.09</td>\n      <td>4168</td>\n      <td>3547</td>\n      <td>2459</td>\n      <td>3768</td>\n      <td>3544</td>\n      <td>1695</td>\n      <td>2862</td>\n      <td>...</td>\n      <td>2282</td>\n      <td>1246</td>\n      <td>1662</td>\n      <td>2120</td>\n      <td>840</td>\n      <td>1336</td>\n      <td>1665</td>\n      <td>1200</td>\n      <td>1191</td>\n      <td>1217</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.359</td>\n      <td>100.11253</td>\n      <td>155.25</td>\n      <td>1239</td>\n      <td>1145</td>\n      <td>1007</td>\n      <td>550</td>\n      <td>1254</td>\n      <td>781</td>\n      <td>1714</td>\n      <td>...</td>\n      <td>203706</td>\n      <td>231624</td>\n      <td>152532</td>\n      <td>231635</td>\n      <td>260914</td>\n      <td>258902</td>\n      <td>234764</td>\n      <td>234498</td>\n      <td>193185</td>\n      <td>193974</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.628</td>\n      <td>101.00802</td>\n      <td>20.59</td>\n      <td>1295</td>\n      <td>1499</td>\n      <td>1508</td>\n      <td>2067</td>\n      <td>2103</td>\n      <td>2153</td>\n      <td>1500</td>\n      <td>...</td>\n      <td>18334</td>\n      <td>14693</td>\n      <td>11126</td>\n      <td>10754</td>\n      <td>14120</td>\n      <td>10229</td>\n      <td>12813</td>\n      <td>10449</td>\n      <td>9869</td>\n      <td>11369</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 157 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "d_ms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = d_ms.rename(columns={'Average Rt(min)': 'Average RT (min)', 'Average Mz': 'Average m/z', 'S/N average': 'Average sn'})\n",
    "d_ms.insert(3, \"Average score\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=['CEC','Blank','ISTD','Wash','Shutdown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(d_input, blank_keyword, svb_thres=10, empty_thres=0, cv_thres=5,rt_range=[0, 30], mz_range=[0, 1200], sn_thres=3, score_thres=0, area_thres=5000):\n",
    "    '''\n",
    "    The function is used to clean the dataframe according to user setting\n",
    "    blank_keyword: part of string from column that indicates the column is a blank sample\n",
    "    svb_thres: sample vs blank thres\n",
    "    empty_thres: empty cell thres in a row\n",
    "    cv_thres: as all sample is in triplicate, calculate the CV for every triplicate sample set #Needs to be updated in case there is no triplicate samples\n",
    "    rt_range: rt filter\n",
    "    mz_range: mz filter\n",
    "    sn_thres: signal/noise column thres\n",
    "    score_thres: score column thres\n",
    "    area_thres: count for max peak area from each row\n",
    "    '''\n",
    "    #Get the index for area thres filter\n",
    "    # DM: can this be written simpler? Why do you need to drop the first index?\n",
    "    drop_index = np.argwhere(np.asarray(d_input[d_input.columns[4:]].max(axis=1)) < area_thres).reshape(1,-1) \n",
    "    d_thres = d_input.drop(drop_index[0])\n",
    "    \n",
    "    d_thres = d_thres[(d_thres['Average RT (min)'] > rt_range[0]) & (d_thres['Average RT (min)'] < rt_range[1])]\n",
    "    d_thres = d_thres[(d_thres['Average m/z'] > mz_range[0]) & (d_thres['Average m/z'] < mz_range[1])]\n",
    "    d_thres = d_thres[d_thres['Average sn'] >= sn_thres]\n",
    "    d_thres = d_thres[d_thres['Average score'] >= score_thres]\n",
    "    d_thres.reset_index(inplace=True)\n",
    "    d_thres.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    col_blank = []\n",
    "    for key in blank_keyword:\n",
    "        # Get column name if it contains blank indicating strings\n",
    "        # DM: can you just use col_blank without col_app?\n",
    "        col_app = [col for col in d_thres.columns if key in col] \n",
    "        col_blank += col_app\n",
    "    col_sample = [col for col in d_thres.columns if col not in col_blank]\n",
    "    # Sample maximum area vs Blank average area to count for svb\n",
    "    d_sample = d_thres[d_thres[col_sample[4:]].max(axis=1) / d_thres[col_blank].mean(axis=1) > svb_thres][col_sample] \n",
    "    d_sample.reset_index(inplace=True)\n",
    "    d_sample.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    # Get a list of triplicate, every triplicate is in a sublist\n",
    "    #Sample: [[a1,a2,a3],[b1,b2,b3]]\n",
    "    #Note: the triplicate parsing is now only used '_' which needs update in the future\n",
    "    trip_list = [list(i) for j, i in groupby(d_sample.columns[4:], lambda a: a.split('_')[1])] \n",
    "\n",
    "    for triplicate in tqdm(trip_list):\n",
    "        # DM: maybe use iterrtuples? iterrows has low efficiency and is not reccomended \n",
    "        for index, row in d_sample[triplicate].iterrows(): # Loop for every sets of triplicates\n",
    "            if (row == 0).sum() > empty_thres:\n",
    "                d_sample.loc[index, triplicate] = 0 # if more than thres, then set all three values to 0\n",
    "            elif row.std() / row.mean() > cv_thres:\n",
    "                d_sample.loc[index, triplicate] = 0 #If delete or reduce all number to avg?\n",
    "            else:\n",
    "                pass\n",
    "    #d_sample = d_sample[(d_sample.iloc[:,4:]!=0).sum(1) > 3]\n",
    "    \n",
    "    \n",
    "    return d_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10/10 [02:26<00:00, 14.65s/it]\n"
    }
   ],
   "source": [
    "d_sample = data_prep(d_ms,keys,rt_range = [1,30], mz_range = [200,800], area_thres=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_cluster(d_input, select_keyword, normalization='linear', visual=False, d_reduce=True, d_reduce_method='tsne', perplexity=20, cluster_method='dbscan',eps=0.8,min_samples=10):\n",
    "    '''\n",
    "    Function for direct clustering:\n",
    "    normalization method: linear, zscore, log\n",
    "    d_reduce: if perform the dimension reduction algorithm, method: only tsne is avilable now\n",
    "    perplexity: parameter for tsne\n",
    "    cluster_method: dbscan, later will update optic and spectrum\n",
    "    eps: parameter for dbscan, threshold of radius that used to count neighbours\n",
    "    min_samples: general parameter for clustering, min neighbourhoods to be counted as a cluster\n",
    "    '''\n",
    "    col_select = []\n",
    "    # DM: just use col_select instead of col_app?\n",
    "    for key in select_keyword:\n",
    "        col_app = [col for col in d_input.columns if key in col]\n",
    "        col_select += col_app\n",
    "    d_clu = d_input[col_select]\n",
    "    \n",
    "    c_data = d_clu.values\n",
    "    c_norm = []\n",
    "    #Performs normalization\n",
    "    for row in c_data:\n",
    "        if normalization == 'linear':\n",
    "            c_norm.append(row/max(row))\n",
    "        elif normalization == 'zscore':\n",
    "            c_norm.append((row-np.mean(row))/np.std(row))\n",
    "        elif normalization == 'log':\n",
    "            row[row==0]=1\n",
    "            c_norm.append(np.log10(row)/np.log10(max(row)))\n",
    "        else:\n",
    "            pass\n",
    "    #Clean up dataframe\n",
    "    c_norm = np.asarray(c_norm)\n",
    "    d_norm = pd.DataFrame(c_norm)\n",
    "    d_norm['index']=d_sample.index\n",
    "    d_norm.set_index('index',inplace=True)\n",
    "    d_norm.dropna(how='all',inplace=True)\n",
    "    \n",
    "    if d_reduce == True:\n",
    "        if d_reduce_method == 'tsne':\n",
    "            # DM: Maybe avoid using X as variable name?\n",
    "            model = TSNE(learning_rate=100,perplexity=50,n_iter=1000) #Tune perplexity and n_iter\n",
    "            transformed = model.fit_transform(d_norm)\n",
    "            X=transformed.copy()\n",
    "        else:\n",
    "            pass\n",
    "    elif d_reduce == False:\n",
    "        # DM: rename for clarity?\n",
    "        X=d_norm.copy()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if cluster_method == 'dbscan':\n",
    "        dbscan = cluster.DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "        labels = dbscan.labels_\n",
    "        unique_labels = set(dbscan.labels_)\n",
    "        \n",
    "        if visual == True:\n",
    "            for i,k in enumerate(unique_labels):\n",
    "                indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "                sns.clustermap(d_norm.iloc[indexlist].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "                plt.title(str(dbscan)+'label='+ str(k))\n",
    "                plt.show()\n",
    "        else:\n",
    "            pass\n",
    "        d_init = d_sample.copy()\n",
    "        d_label = d_init.loc[d_norm.index] #Use the index to match back to the original datasheet\n",
    "        d_label.insert(4,\"label\", dbscan.labels_.tolist())\n",
    "    elif cluster_method == 'optics':\n",
    "        optics = cluster.OPTICS(min_samples=min_samples).fit(X)\n",
    "        labels = optics.labels_\n",
    "        unique_labels = set(optics.labels_)\n",
    "        if visual == True:\n",
    "            for i,k in enumerate(unique_labels):\n",
    "                indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "                sns.clustermap(d_norm.iloc[indexlist].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "                plt.title(str(optics)+'label='+ str(k))\n",
    "                plt.show()\n",
    "        else:\n",
    "            pass\n",
    "        d_init = d_sample.copy()\n",
    "        d_label = d_init.loc[d_norm.index] #Use the index to match back to the original datasheet\n",
    "        d_label.insert(4,\"label\", optics.labels_.tolist())\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #Post filter -- filter out features that present in other sources but not SR520 -- keep it open for now\n",
    "    #If activate add one more variable:source_keyword\n",
    "#     col_source = []\n",
    "#     for key in source_keyword:\n",
    "#         col_app = [col for col in d_thres.columns if key in col]\n",
    "#         col_source += col_app\n",
    "#     col_rest = [col for col in d_label.columns if col not in source][5:]\n",
    "#     d_label[col_app].max(1) / d_label[col_rest].max(1)\n",
    "    \n",
    "    return d_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_label = ms_cluster(d_sample, ['SR520-Cal'], 'linear', d_reduce=False, visual=False, cluster_method='dbscan', eps=0.6, min_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options: all_data/clusters seperately\n",
    "# models: multiple linear/ random forest/ etc..\n",
    "# def modeling:\n",
    "#     select option\n",
    "#     select model\n",
    "#     if option all_data:\n",
    "#         model.fit(data) --> training 1114data, test 0815data\n",
    "#     elif option cluster:\n",
    "#         for group in cluster:\n",
    "#             model.fit(group)\n",
    "#         all_model -- > final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post filtering of dilution cluster\n",
    "# source tracking:\n",
    "#     1. samples from different sites\n",
    "#     2. vann diagram--> 'source subtraction' --> unique features for different source\n",
    "#     3. use cluster/noise distinguish method --> remove noises, get clusters\n",
    "#     4. source proportioning prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Clustering Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_calc(d_input, select_keyword, min_size=5, normalization='linear', visual=True):\n",
    "    \"\"\"This function calculates clustering based on the pearson correlation.\n",
    "    It takes in a dataframe and a user defined value for what qualifies as a cluster.\n",
    "    User can choose whether or not to have a visual plot of the scatter with True/False.\"\"\"\n",
    "    col_select = []\n",
    "    for key in select_keyword:\n",
    "        col_app = [col for col in d_input.columns if key in col]\n",
    "        col_select += col_app\n",
    "    d_clu = d_input[col_select]\n",
    "    \n",
    "    c_data = d_clu.values\n",
    "    c_norm = []\n",
    "    for row in c_data:\n",
    "        if normalization == 'linear':\n",
    "            c_norm.append(row/max(row))\n",
    "        elif normalization == 'zscore':\n",
    "            c_norm.append((row-np.mean(row))/np.std(row))\n",
    "        elif normalization == 'log':\n",
    "            row[row==0]=1\n",
    "            c_norm.append(np.log10(row)/np.log10(max(row)))\n",
    "    c_norm = np.asarray(c_norm)\n",
    "    d_norm = pd.DataFrame(c_norm)\n",
    "    d_norm['index']=d_sample.index\n",
    "    d_norm.set_index('index',inplace=True)\n",
    "    d_norm.dropna(how='all',inplace=True)\n",
    "    \n",
    "    #Post treatment to fit the d_norm into original codes\n",
    "    d_norm.insert(0,\"RT\", d_label['Average RT (min)'].tolist())\n",
    "    d_norm.insert(1,\"MZ\", d_label['Average m/z'].tolist())\n",
    "    d_norm = d_norm.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    #Original codes\n",
    "    cluster = [] # individual cluster holder\n",
    "    cluster_sum = [] # total clusters\n",
    "    drop_list = [] # rows that are dropped from the df\n",
    "    noise = [] # list for containing noise features\n",
    "    while len(d_norm) > 0:\n",
    "        for row in range(len(d_norm)):\n",
    "            feature_1 = d_norm.iloc[0]\n",
    "            feature_2 = d_norm.iloc[row]\n",
    "            corr, p_val = scipy.stats.pearsonr(d_norm.iloc[0, 2:], d_norm.iloc[row, 2:]) #Potentially you can take the 2: off as d_norm.iloc[0] vs d_norm.iloc[row] \n",
    "            #And keep the index the same but not reset it, so you can use the index to link back to the d_input\n",
    "            if p_val < 0.05:\n",
    "                drop_list.append(row)\n",
    "                cluster += [feature_2]\n",
    "            else:\n",
    "                pass\n",
    "        if len(cluster) <= min_size:\n",
    "            noise += [cluster]\n",
    "            cluster = []\n",
    "        else:\n",
    "            cluster_sum += [cluster]\n",
    "            cluster = []\n",
    "        d_norm = d_norm.drop(drop_list)\n",
    "        d_norm = d_norm.reset_index(drop=True)\n",
    "        drop_list = []\n",
    "    append_list = []\n",
    "    for i in range(len(cluster_sum)):\n",
    "        for j in range(len(cluster_sum[i])):\n",
    "            cluster_sum[i][j].loc['Score']= i\n",
    "            listing = np.array(cluster_sum[i][j])\n",
    "            append_list.append(listing)\n",
    "    cluster_df = pd.DataFrame(append_list) #Add columns use d_clu\n",
    "    append_list2 = []\n",
    "    for k in range(len(noise)):\n",
    "        for l in range(len(noise[k])):\n",
    "            noise[k][l].loc['Score']= -1\n",
    "            listing2 = np.array(noise[k][l])\n",
    "            append_list2.append(listing2)\n",
    "    noise_df = pd.DataFrame(append_list2)\n",
    "    final_df = pd.concat([cluster_df, noise_df])\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    if visual == True:\n",
    "        labels = final_df.iloc[:,-1:].values.reshape(1,-1)[0]\n",
    "        unique_labels = set(labels)\n",
    "        for i,k in enumerate(unique_labels):\n",
    "            indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "            sns.clustermap(final_df.iloc[indexlist,2:-1].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "            plt.title('trend'+'label='+ str(k))\n",
    "            plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Trend Cluster Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_1=trend_calc(d_sample, ['SR520-Cal'], min_size=5, normalization='zscore', visual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = pd.read_csv('../example_data/clustering/sample0815.csv')\n",
    "#merge d_test with d_label-->with d_model !=-1\n",
    "#only retain rows that presents in d_label and keep all subsequent the same\n",
    "#Check the alignment.py for ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = d_label[d_label['label']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling using all features except noises\n",
    "d_model = d_label[d_label['label']!=-1]\n",
    "col_model = [col for col in d_model.columns if 'SR520-Cal' in col]\n",
    "d_model = d_model[col_model].T\n",
    "d_model.reset_index(inplace=True)\n",
    "d_model = d_model.rename(columns={'index':'dilu_vol'})\n",
    "d_model['dilu_vol'] = d_model['dilu_vol'].apply(lambda x : float(x.split('_')[2][:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = d_model.iloc[: , 1:]\n",
    "y_train = d_model['dilu_vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 1832 and input n_features is 1870 ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-5cf7528267ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Fit on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \"\"\"\n\u001b[1;32m--> 629\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    397\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 1832 and input n_features is 1870 "
     ]
    }
   ],
   "source": [
    "#Ref: selflearning/direct-modeling.ipynb\n",
    "#Due to small sample size, maybe consider decision tree or random forest rather than knn\n",
    "#Model all features at one step:\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) #How to deal with the data shape? -- align old data and new data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New function: to merge two data together from different batches -- similar to alignment but should be easier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}