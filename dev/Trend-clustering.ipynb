{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster,mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = pd.read_csv('../example_data/clustering/sample1114.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = d_ms.rename(columns={'Average Rt(min)': 'Average RT (min)', 'Average Mz': 'Average m/z', 'S/N average': 'Average sn'})\n",
    "d_ms.insert(3, \"Average score\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_clean(dataframe, rt_range=[0, 30], mz_range=[0, 1200], sn_thres=3, score_thres=0, area_thres=5000): #Update with *args or **args in future updates\n",
    "    #Area thres update\n",
    "    drop_index = np.argwhere(np.asarray(dataframe[dataframe.columns[4:]].max(axis=1)) < area_thres).reshape(1,-1)\n",
    "    df_c = dataframe.drop(drop_index[0])\n",
    "    \n",
    "    df_c = df_c[(df_c['Average RT (min)'] > rt_range[0]) & (df_c['Average RT (min)'] < rt_range[1])]\n",
    "    df_c = df_c[(df_c['Average m/z'] > mz_range[0]) & (df_c['Average m/z'] < mz_range[1])]\n",
    "    df_c = df_c[df_c['Average sn'] >= sn_thres]\n",
    "    df_c = df_c[df_c['Average score'] >= score_thres]\n",
    "    df_c.reset_index(inplace=True)\n",
    "    df_c.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    return df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = d_clean(d_ms,rt_range = [1,30], mz_range = [200,800], area_thres=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinguish between sample and blank\n",
    "col_blank = [col for col in df_c.columns if 'CEC' in col or 'Blank' in col or 'ISTD' in col or 'Wash' in col or 'Shutdown' in col]\n",
    "col_sample = [col for col in df_c.columns if col not in col_blank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample vs blank threshold\n",
    "samplevsblank_thres = 10\n",
    "d_sample = df_c[df_c[col_sample[4:]].max(axis=1) / df_c[col_blank].mean(axis=1) > samplevsblank_thres][col_sample]\n",
    "d_sample.reset_index(inplace=True)\n",
    "d_sample.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise removal from triplicates\n",
    "trip_list = [list(i) for j, i in groupby(d_sample.columns[4:], lambda a: a.split('_')[1])] #Needs to define 1. parser 2. position of parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_tol = 0\n",
    "cv_tol = 5\n",
    "\n",
    "for triplicate in tqdm(trip_list):\n",
    "    for index, row in d_sample[triplicate].iterrows():\n",
    "        if (row == 0).sum() > empty_tol:\n",
    "            d_sample.loc[index, triplicate] = 0\n",
    "            #Filling the gaps and check variance? --- coefficient of variation\n",
    "        elif row.std() / row.mean() > cv_tol:\n",
    "            d_sample.loc[index, triplicate] = 0 #If delete or reduce all number to avg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double check if any empty columns and clean up -- deal with all samples\n",
    "d_sample = d_sample[(d_sample.iloc[:,4:]!=0).sum(1) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with only dilution series\n",
    "col_di = [col for col in d_sample.columns if 'SR520-Cal' in col]\n",
    "d_dilu = d_sample[col_di]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "#Normalization to 0 1 scale\n",
    "#c_data = d_sample[4:].values #returns a numpy array\n",
    "c_data = d_dilu.values\n",
    "\n",
    "\n",
    "#Normalized to absolute values\n",
    "c_norm = []\n",
    "for row in c_data:\n",
    "    c_norm.append(row/max(row))\n",
    "c_norm = np.asarray(c_norm)\n",
    "#Normalized to relative values\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(c_data.T)\n",
    "# df=pd.DataFrame(x_scaled)\n",
    "d_norm = pd.DataFrame(c_norm)\n",
    "d_norm.dropna(how='all',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_calc(df):\n",
    "    cluster = []\n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "    cl = []\n",
    "    df = df.reset_index(drop=True)\n",
    "    for row in range(len(df)):\n",
    "        feature_1 = df.iloc[0]\n",
    "        feature_2 = df.iloc[row]\n",
    "        corr, p_val = scipy.stats.pearsonr(feature_1, feature_2)\n",
    "        if p_val < 0.05:\n",
    "            cl.append(row)\n",
    "            cluster += [feature_2]\n",
    "        else:\n",
    "            pass\n",
    "    df = df.drop(cl)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trend_calc(d_norm)\n",
    "# b=trend_calc(a)\n",
    "# c=trend_calc(b)\n",
    "# trend_calc(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend(df):\n",
    "    total_cluster = []\n",
    "    while (len(df))>0:\n",
    "        total_cluster = [trend_calc(df)]\n",
    "    return total_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend(d_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no 0 is acceptable in the clustermap -- eye balling estimation?\n",
    "sns.clustermap(d_norm.values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False)\n",
    "plt.title('Clustermap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# whole batch or only dilution series\n",
    "whole batch to start with the testing, then apply extra filter to filter out cases, one exist both in source and non-source sample and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization of the dataset for statistical analysis -- unsupervised machine learning\n",
    "#Q: is the normalization needed to terminate the effect of peak area variation?\n",
    "#option: random forest, som, pca+k-means， t-sne+dbscan, autoencoder\n",
    "#option2: non-parametric test\n",
    "#Normalized data-c_data\n",
    "d_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering visualization sample -- scatter plot\n",
    "X=d_norm\n",
    "#msfit = ms.fit(X)\n",
    "db = DBSCAN(eps=0.9, min_samples=5).fit(X)\n",
    "\n",
    "d_label = d_sample.iloc[d_norm.index]\n",
    "d_label['label'] = db.labels_\n",
    "\n",
    "#Plot\n",
    "unique_labels = set(d_label['label'])\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "        \n",
    "    plt.plot(d_label[d_label['label']==k]['Average RT (min)'], d_label[d_label['label']==k]['Average m/z'], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k')\n",
    "    #plt.colorbar()\n",
    "plt.xlabel('rt')\n",
    "plt.ylabel('mz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Clustering visualization sample -- trend plot\n",
    "X=d_norm.copy()\n",
    "#msfit = ms.fit(X)\n",
    "db = DBSCAN(eps=0.9, min_samples=5).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "#Plot\n",
    "unique_labels = set(db.labels_)\n",
    "\n",
    "for i,k in enumerate(unique_labels):\n",
    "    indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "    sns.clustermap(X.iloc[indexlist].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "    plt.title('Clustermap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-para testing ideas -- hypothesis testing\n",
    "similar to alignment, compare row to row trend/statistical difference and then assign neighbours as same group\n",
    "\n",
    "ref:https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = d_norm.iloc[1]\n",
    "data2 = d_norm.iloc[500]\n",
    "plt.scatter(d_norm.iloc[4], d_norm.iloc[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson’s Correlation Coefficient\n",
    "from scipy.stats import pearsonr\n",
    "count = 1\n",
    "stat, p = pearsonr(data1, data2)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    count += 1\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Spearman's Rank Correlation Test\n",
    "from scipy.stats import spearmanr\n",
    "stat, p = spearmanr(data1, data2)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Kendall's Rank Correlation Test\n",
    "from scipy.stats import kendalltau\n",
    "stat, p = kendalltau(data1, data2)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Chi-Squared Test\n",
    "from scipy.stats import chi2_contingency\n",
    "table = [data1,data2]\n",
    "stat, p, dof, expected = chi2_contingency(table)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Mann-Whitney U Test --check for distribution\n",
    "from scipy.stats import mannwhitneyu\n",
    "stat, p = mannwhitneyu(data1, data2)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Kruskal-Wallis H Test\n",
    "from scipy.stats import kruskal\n",
    "stat, p = kruskal(data1, data2)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing data_pairwise\n",
    "if they are similar --> assign to cluster\n",
    "elif not --> assign a new clutser\n",
    "\n",
    "similar to alignment, do we wanna update the cluster information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison plot -- different algorithm with clustering result as color label in the mz/rt scatter plot\n",
    "#Post filter--some cpd show up in dilution but not other samples, and vice versa\n",
    "#Prediction model based on clustering information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
